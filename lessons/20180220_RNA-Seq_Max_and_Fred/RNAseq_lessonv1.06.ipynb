{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNAseq lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this RNA-seq lesson, we will give a basic rundown of procces of going form your raw data to differential gene expression. For this we will use a couple of different tools:<br> \n",
    "<br>\n",
    "On a <b>Mac</b> or <b>linux</b> computer we just need <a href=\"https://www.rstudio.com/products/rstudio/download/\">Rstudio</a> and we need\n",
    "  <a href=\"https://www.anaconda.com/download/#macos\">anaconda</a>\n",
    "from which we will get the other packages.\n",
    ". <br>\n",
    "While on a <b>windows</b> computer you will need to get \n",
    " <a href=\"http://hannonlab.cshl.edu/fastx_toolkit/download.html\">Fastx_toolkit</a>,\n",
    " <a href=\"https://pachterlab.github.io/kallisto/download\">kallisto</a>\n",
    "and \n",
    " <a href=\"https://pachterlab.github.io/sleuth/download\">sleuth</a>\n",
    "from their own websites.<br>\n",
    "\n",
    "<b>Anaconda</b> - This will allow us to get the other packages easy without hickups. be sure to add the \"bioconda\" channel. After which packages can be found by going to \"environments\" then search package your package of choice. from here install \"fastx_toolkit\", \"kallisto\" and \"sleuth\"<br>\n",
    "<b>FASTQC</b>  - to check the quality of the reads. This can be gotten from anaconda. We will not go in depth how to work with this package here, as a previous lesson already covers the subject: <a href=\"http://www.datacarpentry.org/wrangling-genomics/00-quality-control/\">here</a><br>\n",
    "<b>FASTX-toolkit</b> - A toolkit to filter your .fasta files in multiple ways. An alternative is trimmomatic which has many of the same features. Can be gotten from anaconda.<br>\n",
    "<b>Kallisto</b> - This is the alligner we will be using during this example. Can be gotten from anaconda. <br>\n",
    "<b>Sleuth</b> - the accompanying software for Kallisto, this can be run from R(studio) or the shell, we will use Rstudio in the example, if your don't have Rstudio already it can also be gotten from anaconda. How to install it will be covered when we get there.<br>\n",
    "<br>\n",
    "For each step we have providede example data, that can be downloaded from here \n",
    "<a href=\"https://www.dropbox.com/sh/lrl7q4c6yv4y3va/AABsQTqVj47a5C_o8MxL-n2xa?dl=0\">here</a>. If you save all the data to a folder, and set this as your \"working directory\" all command should be able to run.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have provided are Arabidopsis samples from Max his experiment. In which he is comparing different PLC mutants their roots and shoots, under control and drought conditions.<br> In order to keep this lesson concise we have only provided you with 1% of the data (100.000 reads/sample) of Wt shoot samples under control and drought conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastQC\n",
    "We will not delve into the possibilities of FastQC here, for more information you can check out <a href=\"http://www.datacarpentry.org/wrangling-genomics/00-quality-control/\">This</a> lesson.<br>\n",
    "In anaconda you can load the FASTQC package or download it <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\">here</a><br>\n",
    "The raw data files have some reads that have a low quality score, and some reads that are to small to allign properly to the transcripts, so the next step will be to remove these reads.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do 2 things with our sample, we want to remove all reads that are smaller than 25 nucleotides and have a quality score of 17 or less. We choose 17 in this case since we are working with data sequenced using ion-torrent sequencing. If your own data set has used some other sequencing platform you might want to use a different cut off value (illumina is around 30, but very much depended on your samples.)<br>\n",
    "We will use the fastx-toolkit to do this. Make sure you have it installed first. And make sure you are working in the right folder (this should be the folder where the .ipynb file is located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get rid of reads smaller than 25 nucleotides with the following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls\n",
    "fastx_clipper -l 25 -i fastq-files/S01*.fastq -o filtered/S01*.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner we can filter out the reads with low quality scores. In this command the q stands for the quality that we want and the p stands for the percentage of the reads that has to be this quality to be cut out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "fastq_quality_filter -q 17 -p 75 -i fastq-files/S01*.fastq -o filtered/S01*.fastq\n",
    "#note that this will overwrite our previous filtered file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know both commands we can put them together and loop them so we do the filtering for all .fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd fastq-files\n",
    "for filename in S*.fastq; \n",
    "    do fastx_clipper -l 25 -i $filename | fastq_quality_filter -q 17 -p 75 -o ../filtered/$filename-filtered.fastq; \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kallisto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered the data we can use it to allign to a reference genome. For this we will use kallisto. It is a fast and precise allginer.<br>\n",
    "<br>\n",
    "Kallisto can be run with just a single command. It requires: <br>\n",
    "-i a reference genome<br>\n",
    "-o a place to put the output folder<br>\n",
    "-b the number of bootstraps kallist will perform (the number of times the allignemnt will be refined)<br>\n",
    "-- single (or paired if you have paired reads)<br>\n",
    "-l the avarge length of your reads<br>\n",
    "-s the standard deviation of your reads<br>\n",
    "and lastly the file you want to allign.<br>\n",
    "Optionally you can add \"--pseudobam\" before giving the file you want to allign. This wil give the output also in the .bam format, which contains the information of where exactly each read is alligned.<br>\n",
    "<br>\n",
    "The -l and -s can be extracted from you data set using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd filtered\n",
    "for filename in *filtered.fastq; do cat $filename | awk '{if(NR%4==2) print length($1)}' > readlength/$filename.txt; done\n",
    "#this will count the number of characters in every 4th row (the row with the basepairs) and print them in an output file\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these files can now be loaded into R. Where you can extract the mean and SD. <br>\n",
    "#Be sure to first set your working directory. This proccs, I am sure can be looped but I can't for the life of me figure out how. so we will just use the power op \"copy\" and \"paste\" \n",
    "\n",
    "#Do this in R!\n",
    "x1 <- read.csv(\"filtered/readlength/S01subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "x2 <- read.csv(\"filtered/readlength/S02subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "x3 <- read.csv(\"filtered/readlength/S03subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "x4 <- read.csv(\"filtered/readlength/S04subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "x5 <- read.csv(\"filtered/readlength/S05subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "x6 <- read.csv(\"filtered/readlength/S06subset.fastq-filtered.fastq.txt\", header = FALSE)\n",
    "mean(x1$V1); sd(x1$V1)\n",
    "mean(x2$V1); sd(x2$V1)\n",
    "mean(x3$V1); sd(x3$V1)\n",
    "mean(x4$V1); sd(x4$V1)\n",
    "mean(x5$V1); sd(x5$V1)\n",
    "mean(x6$V1); sd(x6$V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#first we need to make an index. This file can get quite big (1.02gb). \n",
    "#A star will appear in the top right corner of this window. when it become a number it is done making the file (5 minutes)\n",
    "kallisto index -i TAIR10_cdna_20101214_updated.idx TAIR10_cdna_20101214_updated.txt\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#in our case the read length and SD is different in every sample, so we will copy paste the command\n",
    "#running this might take some time, as the library made by kallisto is that of the whole arabidopsis genome everytime.\n",
    "cd filtered\n",
    "ls\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S01 -b 5 --single -l 97.143 -s 37.184 S01subset.fastq-filtered.fastq\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S02 -b 5 --single -l 88.706 -s 36.975 S02subset.fastq-filtered.fastq\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S03 -b 5 --single -l 86.015 -s 37.300 S03subset.fastq-filtered.fastq\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S04 -b 5 --single -l 109.709 -s 34.396 S04subset.fastq-filtered.fastq\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S05 -b 5 --single -l 92.368 -s 34.502 S05subset.fastq-filtered.fastq\n",
    "kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/S06 -b 5 --single -l 94.864 -s 37.371 S06subset.fastq-filtered.fastq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# if all your read lengths and sd are the same (this will happen in the case of illumina) you can loop it.\n",
    "cd filtered\n",
    "ls\n",
    "for filename in S01*filtered.fastq; \n",
    "do kallisto quant -i ../TAIR10_cdna_20101214_updated.idx -o ../alligned/$filename -b 5 --single -l 90 -s 30 $filename\n",
    "echo \"done with\" $filename\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our allignements we can find how many reads have alligned to every transcript in the \"abundance.tsv\" files. You can check out the differential expression in your favorite tool, including excel. But we will continue in Sleuth as it is uniquely capable of using the bootstrapping information (abundance.h5) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally if you made the BAM files and what to use these for the <u>UCSC</u> analysis, then you can use the following lines  to get the data ready to be uploaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samtools view -b {input.bam} | bedtools genomecov -split -bg -ibam stdin -g {input.genome} > {output.bg}\n",
    "bedSort {input.bg} {output.sorted.bg}\n",
    "bedGraphToBigWig {input.bedgraph} {input.chroms} {output.bw}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sleuth is a package that is run in R. Sleuth can compare your sample in 2 ways, using a likelyhood ratio test(lrt), or a wald test(wt). The Wald test will give you a fold change between genes, so that is the test we will use in this case.<br>\n",
    "We will now go step by step through the procces of doing a wt test. You can copy each piece into R after each other so you can see what everyhting does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do this in R!\n",
    "#If you haven't already. first install Sleuth. Just copy paste the following command in R and you should be good to go.\n",
    "source(\"http://bioconductor.org/biocLite.R\")\n",
    "biocLite(\"rhdf5\")\n",
    "install.packages(\"devtools\")\n",
    "devtools::install_github(\"pachterlab/sleuth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do this in R!\n",
    "\n",
    "#load sleuth\n",
    "suppressMessages({\n",
    "  library(\"sleuth\")\n",
    "})\n",
    "\n",
    "#set work directory yourself first\n",
    "setwd(\"~/Documents/RNA-seq/lesson\")\n",
    "\n",
    "#make folders to save to\n",
    "dir.create(file.path(\"sleuth_outputWT\"))\n",
    "\n",
    "#Variables. You can make a lot of variables in this run, so you have to change less and less when you want to compare your samples in a different way.\n",
    "#in this case we will use only 2. The thing we want to compare (here options would be: genotype, condition, tissue, mutant)\n",
    "variable <- \"condition\"\n",
    "#and the thing you want to set as a baseline for your other samples to compare to, often this is WT or control\n",
    "baseline <- \"control\"\n",
    "\n",
    "#First Sleuth needs to know where the files are located.\n",
    "sample_id <- dir(file.path(\"alligned\"))\n",
    "\n",
    "#Next we make a file in which the filepaths are located.\n",
    "kal_dirs <- file.path(\"alligned\", sample_id)\n",
    "\n",
    "#We give sleuth the experimental set up\n",
    "s2c <- read.table(file.path(\"experimental_setup.txt\"), header = TRUE, stringsAsFactors=FALSE)\n",
    "#We tell it over which variable to find the difference. In this case it will “condition”\n",
    "s2c <- dplyr::select(s2c, sample = sample, variable)\n",
    "#add filepath for files in pathname so R knows which file corresponds to which metadata entry\n",
    "s2c <- dplyr::mutate(s2c, path = kal_dirs)\n",
    "\n",
    "#You can visualize the addition by looking at the table\n",
    "s2c\n",
    "\n",
    "\n",
    "#Before we can do the WT test, we need change the s2c file  so that is can easily be read by sleuth.\n",
    "condfactor <- s2c[,variable]\n",
    "variablebaseline <- paste (variable,baseline, sep = \"\", collapse = NULL)\n",
    "#makes a table for WT test\n",
    "cond <- factor(condfactor)\n",
    "cond <- relevel(cond, ref = baseline)\n",
    "md <- model.matrix(~cond,s2c)\n",
    "colnames(md)[2] <- variablebaseline\n",
    "\n",
    "\n",
    "#Next we make the sleuth object. This is where all the information about the comparison is going to be in. \n",
    "#you will get a warning, this is because R uses only 1 core of your, probably, multicore computer. When running sleuth in the shell this is not the case. But since runnning from R is easier we will use R anyway.\n",
    "so <- sleuth_prep(s2c, extra_bootstrap_summary = TRUE)\n",
    "\n",
    "\n",
    "#sleuth test on differences using the full and reduced model, don't really understand. you have to fill in the \"condition\" so it knows where to look for differences\n",
    "so <- sleuth_fit(so, md, 'full')\n",
    "so <- sleuth_fit(so, ~1, 'reduced')\n",
    "\n",
    "#And then the walt test is done.\n",
    "so <- sleuth_wt(so, variablebaseline, 'full')\n",
    "\n",
    "\n",
    "#If me make a “model” of this sleuth object we can read it.\n",
    " models(so)\n",
    "#make a table of the restults\n",
    "resultsWT_table <- sleuth_results(so, variablebaseline, test_type = baseline)\n",
    "\n",
    "#sleuth gives a “b” value. This is a “bias-estimator”, it is close to a fold change when the variance is low (bootstrap and variance between samples of the same group), but if the variance is high it will lower this number, to give an indication that is not very certain of the result. Furthermore the b value is calculated with a natural logarithm instead of LOG2, which is the standard. We will add the log2 b value next\n",
    "#first take e^x to get rid of the natural logarithm\n",
    "resultsWT_table$b_e <- exp((resultsWT_table$b))\n",
    "#Then transform with log2\n",
    "resultsWT_table$b_e_log2 <- log2(resultsWT_table$b_e)\n",
    "\n",
    "\n",
    "#write the result to a file\n",
    "write.table(resultsWT_table, file.path(\"sleuth_outputWT\",\"WtCvsWtD.txt\"), sep=\"\\t\") \n",
    "#only take the significant part of those files, (q value < 0.05) write them to a new file and show some\" \n",
    "resultsWT_significant <- dplyr::filter(resultsWT_table, qval <= 0.05)\n",
    "write.table(resultsWT_significant, file.path(\"sleuth_outputWT\",\"WtCvsWtDsig.txt\"), sep=\"\\t\", row.names = FALSE) \n",
    "\n",
    "#Lastly we can open up shiny which will allow us to inspect the data in more detail.\n",
    "sleuth_live(so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sleuth object can also be found (Finished_sleuth_analysis.RData)in the lessen plan if you only want to experiment with the shiny visualisation.<br>\n",
    "under maps, PCA, you can see if the samples map together according to the experimental design.<br>\n",
    "under analysis, transcript view, you can look up some of the differntielly expressed genes that are saved in the WtCvsWtDSig.txt file. Examples to ook up are: <br>\n",
    "AT5G46110.1 low under drought <br>\n",
    "AT1G13930.1 high under drought <br>\n",
    "AT5G25980.2 (has lots of techinical variance, probably due to 3 spliceforms, which normally is not a lot but since we are working with a low number of reads it is.)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
